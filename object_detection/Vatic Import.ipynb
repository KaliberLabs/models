{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Vatic Export Format to Tensorflow Format\n",
    "\n",
    "This notebook is a script to take a vatic export and output tensorflow records. It uses this script as a starting point https://github.com/tensorflow/models/blob/master/object_detection/create_pascal_tf_record.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP_FILE =\"labelmap.pbtxt\"\n",
    "LABEL_MAP = \"\"\"\n",
    "item {\n",
    "  id: 1\n",
    "  name: 'plastic-crate'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'egg-carton'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 3\n",
    "  name: 'milk-carton'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 4\n",
    "  name: 'human-head'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 5\n",
    "  name: 'silver-cart'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 6\n",
    "  name: 'table'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 7\n",
    "  name: 'green-grocery-bag'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 8\n",
    "  name: 'green-grocery-bag'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 9\n",
    "  name: 'yellow-grocery-bag'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 10\n",
    "  name: 'not-a-real-object'\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open(LABEL_MAP_FILE, \"w\") as f:\n",
    "    f.write(LABEL_MAP)\n",
    "\n",
    "label_map_dict = label_map_util.get_label_map_dict(LABEL_MAP_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'egg-carton': 2,\n",
       " 'green-grocery-bag': 8,\n",
       " 'human-head': 4,\n",
       " 'milk-carton': 3,\n",
       " 'not-a-real-object': 10,\n",
       " 'plastic-crate': 1,\n",
       " 'silver-cart': 5,\n",
       " 'table': 6,\n",
       " 'yellow-grocery-bag': 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from lxml import etree\n",
    "import tensorflow as tf\n",
    "import PIL.Image\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "cat ~/model1/data/farmstead1/farmstead1-data.pascal/ImageSets/Main/*trainval.txt  \\\n",
    "    > ~/model1/data/farmstead1/farmstead1-data.pascal/ImageSets/Main/all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On image 0 of 4525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/models/object_detection/utils/dataset_util.py:75: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if not xml:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On image 500 of 4525\n",
      "On image 1000 of 4525\n",
      "On image 1500 of 4525\n",
      "On image 2000 of 4525\n",
      "On image 2500 of 4525\n",
      "On image 3000 of 4525\n",
      "On image 3500 of 4525\n",
      "On image 4000 of 4525\n",
      "On image 4500 of 4525\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dict_to_tf_example(data,\n",
    "                       dataset_directory,\n",
    "                       label_map_dict,\n",
    "                       ignore_difficult_instances=False,\n",
    "                       image_subdirectory='JPEGImages'):\n",
    "    \"\"\"Convert XML derived dict to tf.Example proto.\n",
    "    Notice that this function normalizes the bounding box coordinates provided\n",
    "    by the raw data.\n",
    "    Args:\n",
    "      data: dict holding PASCAL XML fields for a single image (obtained by\n",
    "        running dataset_util.recursive_parse_xml_to_dict)\n",
    "      dataset_directory: Path to root directory holding PASCAL dataset\n",
    "      label_map_dict: A map from string label names to integers ids.\n",
    "      ignore_difficult_instances: Whether to skip difficult instances in the\n",
    "        dataset  (default: False).\n",
    "      image_subdirectory: String specifying subdirectory within the\n",
    "        PASCAL dataset directory holding the actual image data.\n",
    "    Returns:\n",
    "      example: The converted tf.Example.\n",
    "    Raises:\n",
    "      ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n",
    "    \"\"\"\n",
    "\n",
    "    full_path = os.path.join(dataset_directory, image_subdirectory, data['filename'])\n",
    "\n",
    "    with tf.gfile.GFile(full_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "        encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "\n",
    "    if image.format != 'JPEG':\n",
    "        raise ValueError('Image format not JPEG')\n",
    "    \n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    width = int(data['size']['width'])\n",
    "    height = int(data['size']['height'])\n",
    "\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    classes = []\n",
    "    classes_text = []\n",
    "    truncated = []\n",
    "    poses = []\n",
    "    difficult_obj = []\n",
    "\n",
    "    for obj in data['object']:\n",
    "        difficult = bool(int(obj['difficult']))\n",
    "    \n",
    "        if ignore_difficult_instances and difficult:\n",
    "            continue\n",
    "\n",
    "        difficult_obj.append(int(difficult))\n",
    "\n",
    "        xmin.append(float(obj['bndbox']['xmin']) / width)\n",
    "        ymin.append(float(obj['bndbox']['ymin']) / height)\n",
    "        xmax.append(float(obj['bndbox']['xmax']) / width)\n",
    "        ymax.append(float(obj['bndbox']['ymax']) / height)\n",
    "        \n",
    "        classes_text.append(obj['name'].encode('utf8'))\n",
    "        \n",
    "        classes.append(label_map_dict[obj['name']])\n",
    "        \n",
    "        truncated.append(int(obj['truncated']))\n",
    "        poses.append(obj['pose'].encode('utf8'))\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(\n",
    "            data['filename'].encode('utf8')),\n",
    "        'image/source_id': dataset_util.bytes_feature(\n",
    "            data['filename'].encode('utf8')),\n",
    "        'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n",
    "        'image/object/truncated': dataset_util.int64_list_feature(truncated),\n",
    "        'image/object/view': dataset_util.bytes_list_feature(poses),\n",
    "    }))\n",
    "    return example\n",
    "\n",
    "\n",
    "def main(output_dir, data_dir, label_map_dict, eval_to_train_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Assumes data dir looks like this\n",
    "\n",
    "    dataDir/\n",
    "        Annotations/\n",
    "        JPEGImages/\n",
    "        ImageSets/\n",
    "            Main/\n",
    "    \n",
    "    set can be train or trainval\n",
    "    \"\"\"\n",
    "    annotations_dir = os.path.join(data_dir, \"Annotations\")\n",
    "    examples_path = os.path.join(data_dir, 'ImageSets', 'Main', 'all.txt')\n",
    "    examples_list = dataset_util.read_examples_list(examples_path)\n",
    "\n",
    "    eval_path = os.path.join(output_dir, \"eval.tfrecords\")\n",
    "    train_path = os.path.join(output_dir, \"train.tfrecords\")\n",
    "\n",
    "    eval_interval = int(eval_to_train_ratio * len(examples_list)) \n",
    "\n",
    "    with tf.python_io.TFRecordWriter(eval_path) as eval_writer, \\\n",
    "         tf.python_io.TFRecordWriter(train_path) as train_writer:\n",
    " \n",
    "        for idx, example in enumerate(examples_list):\n",
    "            if idx % 500 == 0:\n",
    "                print('On image %d of %d' % (idx, len(examples_list)))\n",
    "\n",
    "            path = os.path.join(annotations_dir, example + '.xml')\n",
    "\n",
    "            with tf.gfile.GFile(path, 'r') as fid:\n",
    "                xml_str = fid.read()\n",
    "\n",
    "            xml = etree.fromstring(xml_str)\n",
    "            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n",
    "\n",
    "            tf_example = dict_to_tf_example(data, data_dir, label_map_dict,\n",
    "                                            ignore_difficult_instances=False)\n",
    "\n",
    "            if idx % eval_interval == 0:\n",
    "                eval_writer.write(tf_example.SerializeToString())\n",
    "            else:\n",
    "                train_writer.write(tf_example.SerializeToString())\n",
    "\n",
    "            \n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "main(\"/home/eli/model1/data/\",\n",
    "     \"/home/eli/model1/data/farmstead1/farmstead1-data.pascal/\", label_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval.tfrecords  \u001b[0m\u001b[01;34mfarmstead1\u001b[0m/  \u001b[01;31mfarmstead1-data.tgz\u001b[0m  train.tfrecords  \u001b[01;34mvoc\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls /home/eli/model1/data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
